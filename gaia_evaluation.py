#!/usr/bin/env python3
"""
GAIA è¯„æµ‹è„šæœ¬ - ç®€åŒ–ç‰ˆ
ç›´æ¥è°ƒç”¨ Qwen3 æ¨¡å‹å¯¹ GAIA æ•°æ®é›†è¿›è¡Œè¯„æµ‹
"""

import asyncio
import json
import os
import sys
from datetime import datetime
from typing import List, Dict, Any
import httpx

# æ·»åŠ agent_deployç›®å½•åˆ°è·¯å¾„
sys.path.append(os.path.join(os.path.dirname(__file__), 'agent_deploy'))

# å¯¼å…¥agent
from qwen3_agent.agent import AWorldAgent

# æ–°å¢ï¼šAWorld llm_agentè‡ªåŠ¨ç”¨å·¥å…·çš„è¯„æµ‹æµç¨‹
from aworld.agents.llm_agent import Agent as LLM_Agent
from aworld.core.common import Observation, StatefulObservation
from aworld.core.tool.tool_desc import get_tool_desc
from aworld.config.conf import AgentConfig, ModelConfig
from aworld.core.context.base import Context

QWEN3_API_URL = "https://modelfactory.lenovo.com/app-workspace-172-1749106801220-vscode/v1/chat/completions"

class GaiaEvaluator:
    def __init__(self):
        self.results = []
        self.agent = AWorldAgent()
    
    def load_dataset(self) -> List[Dict[str, Any]]:
        """åŠ è½½GAIAæ•°æ®é›†"""
        dataset_path = "examples/gaia/GAIA/2023/metadata.jsonl"
        
        if not os.path.exists(dataset_path):
            print(f"âŒ æ•°æ®é›†æ–‡ä»¶ä¸å­˜åœ¨: {dataset_path}")
            return []
        
        dataset = []
        try:
            with open(dataset_path, 'r', encoding='utf-8') as f:
                for line in f:
                    if line.strip():
                        item = json.loads(line)
                        dataset.append(item)
            print(f"âœ… æˆåŠŸåŠ è½½ {len(dataset)} æ¡ GAIA æ•°æ®")
            return dataset
        except Exception as e:
            print(f"âŒ åŠ è½½æ•°æ®é›†å¤±è´¥: {e}")
            return []
    
    async def call_agent(self, messages: List[Dict[str, str]]) -> str:
        """é€šè¿‡agentè°ƒç”¨Qwen3"""
        try:
            # æ„é€ è¯·æ±‚å¯¹è±¡
            class MockRequest:
                def __init__(self, messages):
                    self.messages = messages
            
            class MockMessage:
                def __init__(self, role, content):
                    self.role = role
                    self.content = content
            
            # è½¬æ¢æ¶ˆæ¯æ ¼å¼
            mock_messages = []
            for msg in messages:
                mock_messages.append(MockMessage(msg["role"], msg["content"]))
            
            request = MockRequest(mock_messages)
            
            # è°ƒç”¨agent
            response_content = ""
            async for content in self.agent.run(request=request):
                response_content += content
            
            return response_content
        except Exception as e:
            return f"[Agentè°ƒç”¨å¤±è´¥] {e}"
    
    def extract_answer(self, response: str) -> str:
        """ä»æ¨¡å‹å›å¤ä¸­æå–ç­”æ¡ˆ"""
        # å°è¯•æå– <answer> æ ‡ç­¾ä¸­çš„å†…å®¹
        import re
        match = re.search(r'<answer>(.*?)</answer>', response, re.IGNORECASE | re.DOTALL)
        if match:
            return match.group(1).strip()
        
        # å¦‚æœæ²¡æœ‰æ ‡ç­¾ï¼Œå°è¯•æå–æœ€åä¸€æ®µä½œä¸ºç­”æ¡ˆ
        lines = response.strip().split('\n')
        for line in reversed(lines):
            if line.strip() and not line.startswith('#'):
                return line.strip()
        
        return response.strip()
    
    def score_answer(self, predicted: str, correct: str) -> bool:
        """è¯„åˆ†å‡½æ•° - ç®€å•å­—ç¬¦ä¸²åŒ¹é…"""
        predicted = predicted.lower().strip()
        correct = correct.lower().strip()
        
        # ç›´æ¥åŒ¹é…
        if predicted == correct:
            return True
        
        # åŒ…å«å…³ç³»
        if correct in predicted or predicted in correct:
            return True
        
        # æ•°å­—åŒ¹é…ï¼ˆå»é™¤ç©ºæ ¼å’Œæ ‡ç‚¹ï¼‰
        import re
        pred_clean = re.sub(r'[^\w]', '', predicted)
        corr_clean = re.sub(r'[^\w]', '', correct)
        if pred_clean == corr_clean:
            return True
        
        return False
    
    async def evaluate_single_question(self, item: Dict[str, Any]) -> Dict[str, Any]:
        """è¯„æµ‹å•ä¸ªé—®é¢˜"""
        question = item["Question"]
        correct_answer = item["Final answer"]
        task_id = item["task_id"]
        level = item["Level"]
        
        print(f"\nğŸ” è¯„æµ‹é—®é¢˜ {task_id} (Level {level}):")
        print(f"é—®é¢˜: {question[:100]}...")
        
        # å–æ¨ç†æ­¥æ•°
        reasoning_steps = 0
        try:
            steps_str = item.get("Annotator Metadata", {}).get("Number of steps", "0")
            reasoning_steps = int(steps_str)
        except Exception:
            pass
        # å–å·¥å…·åˆ—è¡¨
        tool_list = []
        try:
            tools_str = item.get("Annotator Metadata", {}).get("Tools", "")
            for line in tools_str.splitlines():
                tool = line.strip()
                if tool:
                    # å»æ‰å‰é¢çš„ç¼–å·å’Œç‚¹
                    tool = tool.split('.', 1)[-1].strip()
                    # å»æ‰æ‹¬å·å†…å®¹
                    import re
                    tool = re.sub(r"\\(.*?\\)", "", tool).strip()
                    if tool:
                        tool_list.append(tool)
        except Exception:
            pass
        # å–å·¥å…·æ•°é‡ï¼Œä¼˜å…ˆAnnotator Metadataï¼Œå…¶æ¬¡æœ€å¤–å±‚ï¼Œæœ€åç”¨å·¥å…·åˆ—è¡¨é•¿åº¦å…œåº•
        tool_count = 0
        try:
            tool_count_str = item.get("Annotator Metadata", {}).get("Number of tools")
            if tool_count_str is None:
                tool_count_str = item.get("Number of tools")
            if tool_count_str is not None:
                tool_count = int(tool_count_str)
            # å¦‚æœå·¥å…·åˆ—è¡¨é•¿åº¦å’Œæ•°é‡ä¸ä¸€è‡´ï¼Œä»¥å·¥å…·åˆ—è¡¨é•¿åº¦ä¸ºå‡†
            if tool_count == 0 and tool_list:
                tool_count = len(tool_list)
        except Exception:
            tool_count = len(tool_list)
        group = level
        print(f"  æ¨ç†æ­¥æ•°: {reasoning_steps}")
        print(f"  å·¥å…·æ•°é‡: {tool_count}")
        print(f"  å·¥å…·åˆ—è¡¨: {tool_list}")
        print(f"  åˆ†ç»„(Level): {group}")
        
        # æ„é€ æ¶ˆæ¯
        messages = [
            {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªçŸ¥è¯†æ¸Šåšçš„AIåŠ©æ‰‹ã€‚è¯·ä¸¥æ ¼æŒ‰ç…§å¦‚ä¸‹è¦æ±‚å›ç­”ï¼š1. åªè¾“å‡ºæœ€ç»ˆç­”æ¡ˆï¼Œä¸è¦è¾“å‡ºæ¨ç†è¿‡ç¨‹ã€‚2. ç­”æ¡ˆå¿…é¡»ç”¨<answer>æ ‡ç­¾åŒ…è£¹ï¼Œä¾‹å¦‚<answer>42</answer>ã€‚3. å¦‚æœæ— æ³•å›ç­”ï¼Œè¾“å‡º<answer>æ— æ³•å›ç­”</answer>ã€‚"},
            {"role": "user", "content": question}
        ]
        
        # è°ƒç”¨agent
        response = await self.call_agent(messages)
        predicted_answer = self.extract_answer(response)
        
        # è¯„åˆ†
        is_correct = self.score_answer(predicted_answer, correct_answer)
        
        result = {
            "task_id": task_id,
            "level": level,
            "question": question,
            "correct_answer": correct_answer,
            "predicted_answer": predicted_answer,
            "full_response": response,
            "is_correct": is_correct
        }
        
        status = "âœ…" if is_correct else "âŒ"
        print(f"{status} é¢„æµ‹: {predicted_answer} | æ­£ç¡®ç­”æ¡ˆ: {correct_answer}")
        
        return result
    
    async def run_evaluation(self, max_questions: int = 10):
        """è¿è¡Œå®Œæ•´è¯„æµ‹"""
        print("ğŸš€ å¼€å§‹ GAIA è¯„æµ‹...")
        
        # åŠ è½½æ•°æ®é›†
        dataset = self.load_dataset()
        if not dataset:
            return
        
        # é™åˆ¶è¯„æµ‹æ•°é‡
        if max_questions > 0:
            dataset = dataset[:max_questions]
        
        print(f"ğŸ“Š å°†è¯„æµ‹ {len(dataset)} ä¸ªé—®é¢˜")
        
        # é€ä¸ªè¯„æµ‹
        for i, item in enumerate(dataset, 1):
            try:
                result = await self.evaluate_single_question(item)
                self.results.append(result)
                
                # æ˜¾ç¤ºè¿›åº¦
                correct_count = sum(1 for r in self.results if r["is_correct"])
                accuracy = correct_count / len(self.results) * 100
                print(f"ğŸ“ˆ è¿›åº¦: {i}/{len(dataset)} | å‡†ç¡®ç‡: {accuracy:.1f}% ({correct_count}/{len(self.results)})")
                
                # é¿å…è¯·æ±‚è¿‡å¿«
                await asyncio.sleep(1)
                
            except Exception as e:
                print(f"âŒ è¯„æµ‹é—®é¢˜ {item.get('task_id', 'unknown')} å¤±è´¥: {e}")
                continue
        
        # ç”ŸæˆæŠ¥å‘Š
        self.generate_report()
    
    def generate_report(self):
        """ç”Ÿæˆè¯„æµ‹æŠ¥å‘Š"""
        if not self.results:
            print("âŒ æ²¡æœ‰è¯„æµ‹ç»“æœ")
            return
        
        # ç»Ÿè®¡ç»“æœ
        total = len(self.results)
        correct = sum(1 for r in self.results if r["is_correct"])
        accuracy = correct / total * 100 if total > 0 else 0
        
        # æŒ‰éš¾åº¦ç»Ÿè®¡
        level_stats = {}
        for result in self.results:
            level = result["level"]
            if level not in level_stats:
                level_stats[level] = {"total": 0, "correct": 0}
            level_stats[level]["total"] += 1
            if result["is_correct"]:
                level_stats[level]["correct"] += 1
        
        # ========== æ–°å¢å¤šç»´åº¦ç»Ÿè®¡ ===========
        # 1. å¹³å‡æ¨ç†æ­¥æ•°
        reasoning_steps = []
        # 2. å¹³å‡å·¥å…·ä½¿ç”¨æ•°é‡
        tool_counts = []
        # 3. å·¥å…·ä½¿ç”¨é¢‘ç‡ç»Ÿè®¡
        tool_freq = {}
        # 4. æŒ‰é¢˜å‹åˆ†ç»„å‡†ç¡®ç‡ï¼ˆä»¥Levelä¸ºä¾‹ï¼‰
        group_accuracy = {}
        
        # éœ€è¦é‡æ–°åŠ è½½åŸå§‹æ•°æ®ä»¥æå–å…ƒæ•°æ®
        dataset_path = "examples/gaia/GAIA/2023/metadata.jsonl"
        dataset = []
        if os.path.exists(dataset_path):
            with open(dataset_path, 'r', encoding='utf-8') as f:
                for line in f:
                    if line.strip():
                        item = json.loads(line)
                        dataset.append(item)
        # æ„å»º task_id åˆ°å…ƒæ•°æ®çš„æ˜ å°„
        meta_map = {item.get("task_id"): item for item in dataset}
        
        for result in self.results:
            task_id = result["task_id"]
            meta = meta_map.get(task_id, {})
            # å¹³å‡æ¨ç†æ­¥æ•°
            steps = None
            try:
                steps = meta.get("Annotator Metadata", {}).get("Number of steps")
                if steps is not None:
                    reasoning_steps.append(steps)
            except Exception:
                pass
            # å¹³å‡å·¥å…·ä½¿ç”¨æ•°é‡
            tool_num = None
            try:
                tool_num = meta.get("Number of tools")
                if tool_num is not None:
                    tool_counts.append(tool_num)
            except Exception:
                pass
            # å·¥å…·ä½¿ç”¨é¢‘ç‡
            try:
                tools = meta.get("Tools", [])
                for tool in tools:
                    tool_freq[tool] = tool_freq.get(tool, 0) + 1
            except Exception:
                pass
            # æŒ‰é¢˜å‹åˆ†ç»„å‡†ç¡®ç‡ï¼ˆä»¥Levelä¸ºä¾‹ï¼‰
            group = meta.get("Level", "Unknown")
            group = str(group)  # ä¿è¯groupä¸ºå­—ç¬¦ä¸²ï¼Œé¿å…int+stré”™è¯¯
            if group not in group_accuracy:
                group_accuracy[group] = {"total": 0, "correct": 0}
            group_accuracy[group]["total"] += 1
            if result["is_correct"]:
                group_accuracy[group]["correct"] += 1
        
        avg_reasoning_steps = round(sum(reasoning_steps) / len(reasoning_steps), 2) if reasoning_steps else 0
        avg_tools_used = round(sum(tool_counts) / len(tool_counts), 2) if tool_counts else 0
        
        # ç»„å‡†ç¡®ç‡
        group_acc_json = {k: (v["correct"] / v["total"] if v["total"] else 0) for k, v in group_accuracy.items()}
        
        # è¾“å‡ºJSONç»Ÿè®¡
        stats_json = {
            "overall_accuracy": round(accuracy / 100, 4),
            "avg_reasoning_steps": avg_reasoning_steps,
            "avg_tools_used": avg_tools_used,
            "tool_usage_stats": tool_freq,
            "group_accuracy": group_acc_json
        }
        print("\nğŸ“Š è¯„æµ‹å¤šç»´åº¦ç»Ÿè®¡(JSON):\n" + json.dumps(stats_json, ensure_ascii=False, indent=2))
        
        # ========== ä¿ç•™åŸæœ‰æŠ¥å‘Šè¾“å‡º ===========
        # ç”ŸæˆæŠ¥å‘Š
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        report_file = f"gaia_evaluation_report_{timestamp}.md"
        
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write("# GAIA è¯„æµ‹æŠ¥å‘Š\n\n")
            f.write(f"**è¯„æµ‹æ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
            f.write(f"**æ¨¡å‹**: Qwen3-235B-A22B-FP8\n\n")
            f.write(f"**API**: {QWEN3_API_URL}\n\n")
            
            f.write("## æ€»ä½“ç»“æœ\n\n")
            f.write(f"- **æ€»é¢˜æ•°**: {total}\n")
            f.write(f"- **æ­£ç¡®æ•°**: {correct}\n")
            f.write(f"- **å‡†ç¡®ç‡**: {accuracy:.2f}%\n\n")
            
            f.write("## æŒ‰éš¾åº¦ç»Ÿè®¡\n\n")
            for level in sorted(level_stats.keys()):
                stats = level_stats[level]
                level_accuracy = stats["correct"] / stats["total"] * 100
                f.write(f"### Level {level}\n")
                f.write(f"- é¢˜æ•°: {stats['total']}\n")
                f.write(f"- æ­£ç¡®: {stats['correct']}\n")
                f.write(f"- å‡†ç¡®ç‡: {level_accuracy:.2f}%\n\n")
            
            f.write("## è¯¦ç»†ç»“æœ\n\n")
            for result in self.results:
                status = "âœ…" if result["is_correct"] else "âŒ"
                f.write(f"### {status} {result['task_id']} (Level {result['level']})\n\n")
                f.write(f"**é—®é¢˜**: {result['question']}\n\n")
                f.write(f"**æ­£ç¡®ç­”æ¡ˆ**: {result['correct_answer']}\n\n")
                f.write(f"**æ¨¡å‹ç­”æ¡ˆ**: {result['predicted_answer']}\n\n")
                f.write(f"**å®Œæ•´å›å¤**:\n```\n{result['full_response']}\n```\n\n")
                f.write("---\n\n")
        
        print(f"\nğŸ“‹ è¯„æµ‹æŠ¥å‘Šå·²ç”Ÿæˆ: {report_file}")
        print(f"ğŸ“Š æ€»ä½“å‡†ç¡®ç‡: {accuracy:.2f}% ({correct}/{total})")

class GaiaLlmAgentEvaluator:
    def __init__(self, tool_names=None):
        self.results = []
        # å·¥å…·ååˆ—è¡¨ï¼Œè‡ªåŠ¨è·å–æ‰€æœ‰å·²æ³¨å†Œå·¥å…·
        if tool_names is None:
            tool_names = list(get_tool_desc().keys())
        # æ„é€ æœ€å°å¯ç”¨AgentConfig
        conf = AgentConfig(
            name="llm_agent",
            llm_config=ModelConfig(
                llm_provider="qwen-openai",
                llm_model_name="qwen-turbo",
                llm_api_key="sk-xxx",  # ä½ å¯ä»¥æ”¹æˆè‡ªå·±çš„key
                llm_base_url="https://api.openai.com/v1"
            ),
            system_prompt="ä½ æ˜¯ä¸€ä¸ªå–„äºä½¿ç”¨å·¥å…·çš„AIåŠ©æ‰‹ã€‚",
            agent_prompt="ä½ æ˜¯ä¸€ä¸ªå–„äºä½¿ç”¨å·¥å…·çš„AIåŠ©æ‰‹ã€‚"
        )
        self.agent = LLM_Agent(conf=conf, name="llm_agent", tool_names=tool_names)

    async def call_agent(self, question: str) -> str:
        obs = StatefulObservation(content=question, context=[])
        # llm_agentæ”¯æŒasync_run
        try:
            result = await self.agent.async_run(obs)
            # result.answer å¯èƒ½æ˜¯æœ€ç»ˆç­”æ¡ˆ
            return getattr(result, 'answer', str(result))
        except Exception as e:
            return f"[llm_agentè°ƒç”¨å¤±è´¥] {e}"

    async def evaluate_single_question(self, item: Dict[str, Any]) -> Dict[str, Any]:
        question = item["Question"]
        correct_answer = item["Final answer"]
        task_id = item["task_id"]
        level = item["Level"]
        print(f"\nğŸ” [llm_agent] è¯„æµ‹é—®é¢˜ {task_id} (Level {level}):")
        print(f"é—®é¢˜: {question[:100]}...")
        response = await self.call_agent(question)
        predicted_answer = response.strip()
        is_correct = predicted_answer.lower() == correct_answer.lower()
        result = {
            "task_id": task_id,
            "level": level,
            "question": question,
            "correct_answer": correct_answer,
            "predicted_answer": predicted_answer,
            "full_response": response,
            "is_correct": is_correct
        }
        status = "âœ…" if is_correct else "âŒ"
        print(f"{status} é¢„æµ‹: {predicted_answer} | æ­£ç¡®ç­”æ¡ˆ: {correct_answer}")
        return result

    async def run_evaluation(self, max_questions: int = 10):
        print("ğŸš€ [llm_agent] å¼€å§‹ GAIA å·¥å…·é“¾è¯„æµ‹...")
        dataset_path = "examples/gaia/GAIA/2023/metadata.jsonl"
        dataset = []
        if not os.path.exists(dataset_path):
            print(f"âŒ æ•°æ®é›†æ–‡ä»¶ä¸å­˜åœ¨: {dataset_path}")
            return
        with open(dataset_path, 'r', encoding='utf-8') as f:
            for line in f:
                if line.strip():
                    item = json.loads(line)
                    dataset.append(item)
        if max_questions > 0:
            dataset = dataset[:max_questions]
        print(f"ğŸ“Š å°†è¯„æµ‹ {len(dataset)} ä¸ªé—®é¢˜")
        accuracy = 0.0
        correct_count = 0
        for i, item in enumerate(dataset, 1):
            try:
                result = await self.evaluate_single_question(item)
                self.results.append(result)
                correct_count = sum(1 for r in self.results if r["is_correct"])
                accuracy = correct_count / len(self.results) * 100
                print(f"ğŸ“ˆ è¿›åº¦: {i}/{len(dataset)} | å‡†ç¡®ç‡: {accuracy:.1f}% ({correct_count}/{len(self.results)})")
                await asyncio.sleep(1)
            except Exception as e:
                print(f"âŒ è¯„æµ‹é—®é¢˜ {item.get('task_id', 'unknown')} å¤±è´¥: {e}")
                continue
        print(f"\nğŸ“Š [llm_agent] å·¥å…·é“¾è¯„æµ‹å‡†ç¡®ç‡: {accuracy:.2f}% ({correct_count}/{len(self.results)})")

# ä¸»å‡½æ•°æ”¯æŒä¸¤ç§è¯„æµ‹æ¨¡å¼
async def main():
    import argparse
    parser = argparse.ArgumentParser()
    parser.add_argument('--mode', choices=['qwen3', 'llm_agent'], default='qwen3', help='é€‰æ‹©è¯„æµ‹æ¨¡å¼')
    parser.add_argument('--max_questions', type=int, default=10)
    args = parser.parse_args()
    if args.mode == 'llm_agent':
        evaluator = GaiaLlmAgentEvaluator()
        await evaluator.run_evaluation(max_questions=args.max_questions)
    else:
        evaluator = GaiaEvaluator()
        await evaluator.run_evaluation(max_questions=args.max_questions)

if __name__ == "__main__":
    asyncio.run(main()) 